{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TFP from regression to model",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eznuLqotWD_a",
        "colab_type": "text"
      },
      "source": [
        "# From regression to uncertainty modelling\n",
        "\n",
        "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://colab.research.google.com/github/mtwenzel/teaching/blob/master/TFP_from_regression_to_model.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />Run in Google Colab</a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://github.com/mtwenzel/teaching/blob/master/TFP_from_regression_to_model.ipynb\"><img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\" />View source on GitHub</a>\n",
        "  </td>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MrFWd2qC-Ouy",
        "colab_type": "text"
      },
      "source": [
        "## Legal\n",
        "This code (C) Fraunhofer MEVIS, 2019\n",
        "The code uses examples from TF Probability's original authors, as described in this post: https://medium.com/tensorflow/regression-with-probabilistic-layers-in-tensorflow-probability-e46ff5d37baf, which have been modified by Alessandro Angioi (https://www.angioi.com/tensorflow-heteroscedasticity/). The synthetic data de\n",
        "\n",
        "The code for the latter is available in the original TF GitHub. https://github.com/tensorflow/probability/blob/master/tensorflow_probability/examples/jupyter_notebooks/Probabilistic_Layers_Regression.ipynb"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c9OlkuCjWIvL",
        "colab_type": "text"
      },
      "source": [
        "## Preparation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "JP1Rcocq42Ud",
        "colab": {}
      },
      "source": [
        "#@title Install required libraries; imports {display-mode:'form'}\n",
        "! pip install tensorflow==2.0.0-beta1\n",
        "! pip install tensorflow-probability\n",
        "#@title Imports. {display-mode:'form'}\n",
        "import tensorflow as tf\n",
        "import tensorflow_probability as tfp\n",
        "tfd = tfp.distributions\n",
        "\n",
        "from google.colab import files\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "sns.set()\n",
        "import numpy as np\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ykgEQ2Xo-g6T",
        "colab_type": "text"
      },
      "source": [
        "We create a simple joint distribution, where the scale of the Gaussian depends on the location on the x axis. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UoH8WzyS-r87",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#@title Parameters for the simulation {display-mode:'form', run: 'auto'}\n",
        "\n",
        "#@markdown How many samples to draw from distribution\n",
        "num_samples = 2000 #@param {type: 'slider', min: 1000, max: 10000, step: 500}\n",
        "#@markdown Scale of the Gaussian depends on x axis location; this is an additive component:\n",
        "min_scale = 3 #@param\n",
        "low = -10 #@param {type: 'slider', min: -50, max: -1, step: 1}\n",
        "high = 40 #@param {type: 'slider', min: -1, max: 100, step: 1}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rrhgY2oPgFlp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "joint = tfd.JointDistributionSequential([\n",
        "                 tfd.Uniform(low=low, high=high),\n",
        "      lambda x : tfd.Normal(loc=x, scale=abs(x)+min_scale)\n",
        "])\n",
        "\n",
        "X, Y = joint.sample(num_samples)\n",
        "X = X.numpy().reshape(-1,1)\n",
        "Y = Y.numpy().reshape(-1,1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yLKpp4T7g_WF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#@title Plot {display-mode:'form'}\n",
        "plt.scatter(X,Y, alpha=0.4)\n",
        "plt.xlabel('X')\n",
        "plt.ylabel('Y')\n",
        "#plt.savefig(\"scatter.svg\")\n",
        "#files.download('scatter.svg')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7skpHREuOrcR",
        "colab_type": "text"
      },
      "source": [
        "## Linear regression in TF\n",
        "\n",
        "In TFP, layers can return distributions or expect them as input. To make no assumptions, layers that can return distributions also can return scalars by sampling from the distribution. The framework takes care of what is needed when."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KZnreTSJvilw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#@markdown Confirm that the DistributionLambda behaves like a distribution. \n",
        "layer = tfp.layers.DistributionLambda(lambda t: tfd.Normal(loc=t, scale=1))\n",
        "assert isinstance(layer(2.), tfd.Normal) # It is indeed a distribution.\n",
        "print(layer(2.).sample(5)) # From distributions, one can sample.\n",
        "print(layer(2.).loc)\n",
        "print(layer(2.).stddev())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HkwOq56LizKG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# We define a model already using a distribution as output. Technically, this changes nothing. We still only train one weight and bias, yielding y=wx+b\n",
        "model = tf.keras.Sequential([\n",
        "            tf.keras.layers.Dense(1), # This layer calculates y = wx + b\n",
        "            tfp.layers.DistributionLambda(lambda t: tfd.Normal(loc=t, scale=1)), # This layer takes the output of the previous and constructs a distribution conditioned on the layer inputs\n",
        "])\n",
        "\n",
        "model.compile(optimizer=tf.optimizers.Adam(learning_rate = 0.1),\n",
        "              loss='mean_squared_error')\n",
        "model.fit(X, Y, epochs=40)\n",
        "model.summary()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4ANNlsBv5RM0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#@title Plot {display-mode:'form'}\n",
        "#@markdown Note we plot results of a learned distribution: it's mean. We also plot the standard deviation -- but we haven't learned it, but set to 1 statically.\n",
        "x = np.linspace(low,high).reshape(-1,1)\n",
        "y = model(x)\n",
        "\n",
        "plt.scatter(X,Y, alpha=0.4)\n",
        "plt.plot(x, y.mean(), color = \"red\")\n",
        "plt.plot(x, y.mean() + 2 * y.stddev(), color = \"green\");\n",
        "plt.plot(x, y.mean() - 2 * y.stddev(), color = \"green\");\n",
        "plt.xlabel('X')\n",
        "plt.ylabel('Y')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CwEf8lf3JBcj",
        "colab_type": "text"
      },
      "source": [
        "## Global uncertainty\n",
        "\n",
        "Model the global model uncertainty using two parameters in the dense layer to account for the two outputs: mean and standard deviation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8V4LbJAmu78h",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "dEysHiy4IJ11",
        "colab": {}
      },
      "source": [
        "model = tf.keras.Sequential([\n",
        "            tf.keras.layers.Dense(2), # This layer again calculates y = wx + b, but this time two times independently, yielding a 2-vector.\n",
        "            tfp.layers.DistributionLambda(lambda t: # Interpretation of parameters t as distribution loc/scale (taking the first and second element using t[...,:1] and t[...,1:]).\n",
        "                tfd.Normal(loc   = t[...,:1],\n",
        "                           scale=tf.math.softplus(0.005*t[...,1:])+0.001) # Exponential uncertainty\n",
        "            )\n",
        "])\n",
        "\n",
        "negloglik = lambda y, p_y: -p_y.log_prob(y) # Keras passes the output of the final layer of the model into the loss function, and for the models in this notebook, all those layers return distributions.\n",
        "\n",
        "model.compile(optimizer=tf.optimizers.Adam(learning_rate = 0.1),\n",
        "              loss=negloglik)\n",
        "\n",
        "model.fit(X, Y, epochs=100);\n",
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nr7ThsR1IryK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#@title Plot {display-mode:'form'}\n",
        "\n",
        "x = np.linspace(low,high).reshape(-1,1)\n",
        "y = model(x)\n",
        "\n",
        "plt.scatter(X,Y, alpha=0.4)\n",
        "plt.plot(x, y.mean(), color = \"red\");\n",
        "plt.plot(x, y.mean() + 2 * y.stddev(), color = \"green\");\n",
        "plt.plot(x, y.mean() - 2 * y.stddev(), color = \"green\");\n",
        "plt.xlabel('X')\n",
        "plt.ylabel('Y')\n",
        "\n",
        "#plt.savefig(\"conf1.svg\")\n",
        "#files.download('conf1.svg')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FfbwVD21hnFM",
        "colab_type": "text"
      },
      "source": [
        "## Epistemic uncertainty"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_QQ6Aq4vh_2q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Specify the surrogate posterior over `keras.layers.Dense` `kernel` and `bias`.\n",
        "def posterior_mean_field(kernel_size, bias_size=0, dtype=None):\n",
        "  n = kernel_size + bias_size\n",
        "  c = np.log(np.expm1(1.))\n",
        "  return tf.keras.Sequential([\n",
        "      tfp.layers.VariableLayer(2 * n, dtype=dtype),\n",
        "      tfp.layers.DistributionLambda(lambda t: tfd.Independent(\n",
        "          tfd.Normal(loc=t[..., :n],\n",
        "                     scale=1e-5 + tf.nn.softplus(c + t[..., n:])),\n",
        "          reinterpreted_batch_ndims=1)),\n",
        "  ])\n",
        "\n",
        "# Specify the prior over `keras.layers.Dense` `kernel` and `bias`.\n",
        "def prior_trainable(kernel_size, bias_size=0, dtype=None):\n",
        "  n = kernel_size + bias_size\n",
        "  return tf.keras.Sequential([\n",
        "      tfp.layers.VariableLayer(n, dtype=dtype),\n",
        "      tfp.layers.DistributionLambda(lambda t: tfd.Independent(\n",
        "          tfd.Normal(loc=t, scale=1),\n",
        "          reinterpreted_batch_ndims=1)),\n",
        "  ])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OiTq1UFokN9H",
        "colab_type": "text"
      },
      "source": [
        "The DenseVariational layer used now uses variational inference to fit a \"surrogate\" posterior to the distribution over both the `kernel` matrix and the `bias` terms which are otherwise used in a manner similar to `tf.keras.layers.Dense`.\n",
        "\n",
        "This layer fits the \"weights posterior\" according to the following generative\n",
        "process:\n",
        "\n",
        "  ```none\n",
        "  [K, b] ~ Prior()\n",
        "  M = matmul(X, K) + b\n",
        "  Y ~ Likelihood(M)\n",
        "  ```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0F4NlMAChvHx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = tf.keras.Sequential([\n",
        "            tfp.layers.DenseVariational(1, posterior_mean_field, prior_trainable), # The DenseVariational layer consists of a posterior (two Gaussians) and a Prior (one Gaussian) with an overall of 2*(2n+n) parameters, six in our case (n=1)\n",
        "            tfp.layers.DistributionLambda(lambda t: tfd.Normal(loc=t, scale=1)\n",
        "            )\n",
        "])\n",
        "\n",
        "negloglik = lambda y, p_y: -p_y.log_prob(y)\n",
        "\n",
        "model.compile(optimizer=tf.optimizers.Adam(learning_rate = 0.1),\n",
        "              loss=negloglik)\n",
        "\n",
        "model.fit(X, Y, epochs=100);\n",
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_9CJnAcwngY5",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title Predict\n",
        "x_tst = np.linspace(low,high).reshape(-1,1)\n",
        "yhats = [model(x_tst) for _ in range(100)]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tQLwOZRNl_o3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#@title Plot {display-mode:'form'}\n",
        "plt.figure(figsize=[12, 8])  # inches\n",
        "plt.clf();\n",
        "plt.plot(X, Y, 'b.', label='observed');\n",
        "\n",
        "avgm = np.zeros_like(x_tst[..., 0])\n",
        "for i, yhat in enumerate(yhats):\n",
        "  m = np.squeeze(yhat.mean())\n",
        "  s = np.squeeze(yhat.stddev())\n",
        "  if i < 25:\n",
        "    plt.plot(x_tst, m, 'g', label='ensemble means' if i == 0 else None, linewidth=0.5)\n",
        "  avgm += m\n",
        "plt.plot(x_tst, avgm/len(yhats), 'r', label='overall mean', linewidth=4)\n",
        "\n",
        "plt.ylim(-20,35);\n",
        "\n",
        "ax=plt.gca();\n",
        "ax.xaxis.set_ticks_position('bottom')\n",
        "ax.yaxis.set_ticks_position('left')\n",
        "ax.spines['left'].set_position(('data', 0))\n",
        "ax.spines['top'].set_visible(False)\n",
        "ax.spines['right'].set_visible(False)\n",
        "plt.legend(loc='center left', fancybox=True, framealpha=0., bbox_to_anchor=(1.05, 0.5))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DsWisJ1iIs0D",
        "colab_type": "text"
      },
      "source": [
        "## Introducing nonlinear uncertainty\n",
        "\n",
        "We now model the nonlinear dependency of the uncertainty by introducing more degrees of freedom in the network (additional Dense layer) with a nonlinear activation.\n",
        "\n",
        "Here are some suggestions what to explore after that.\n",
        "\n",
        "* Observe what happens when you change the scale of the Normal distribution to be allowed to go linearly with the x axis, versus going exponentially.\n",
        "* Replace `scale=tf.math.softplus(0.005*t[...,1:])+0.001)` with `scale=tf.exp(0.005*t[...,1:])+0.001)` and run the example again.\n",
        "* Change the range of the example data. Originally, it is set to the interval [-8, 15]. Change it to [-50, 50] and run the experiment again."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OENnNtmX9EM8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = tf.keras.Sequential([\n",
        "            tf.keras.layers.Dense(20,activation=\"relu\"), # Layer creates a nonlinear mapping of input to output\n",
        "            tf.keras.layers.Dense(2), # Layer condensing the knowledge about the input into two parameters\n",
        "            tfp.layers.DistributionLambda(lambda t: # Interpretation of parameters t as distribution loc/scale (taking the first and second element using t[...,:1] and t[...,1:]).\n",
        "                tfd.Normal(loc   = t[...,:1],\n",
        "                           scale=tf.math.softplus(0.005*t[...,1:])+0.001)\n",
        "            )\n",
        "])\n",
        "\n",
        "negloglik = lambda y, p_y: -p_y.log_prob(y)\n",
        "\n",
        "model.compile(optimizer=tf.optimizers.Adam(learning_rate = 0.05),\n",
        "              loss=negloglik)\n",
        "\n",
        "model.fit(X, Y, epochs=100);"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3eWSP7a1OU7f",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#@title Plot {display-mode:'form'}\n",
        "\n",
        "x = np.linspace(low,high).reshape(-1,1)\n",
        "y = model(x)\n",
        "\n",
        "plt.scatter(X,Y, alpha=0.4)\n",
        "plt.plot(x, y.mean(), color = \"red\");\n",
        "plt.plot(x, y.mean() + 2 * y.stddev(), color = \"white\");\n",
        "plt.plot(x, y.mean() - 2 * y.stddev(), color = \"white\");\n",
        "\n",
        "plt.xlabel('X')\n",
        "plt.ylabel('Y')\n",
        "#plt.savefig(\"conf2.svg\")\n",
        "\n",
        "#files.download('conf2.svg')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0s7gL5QO2DnB",
        "colab_type": "text"
      },
      "source": [
        "## What's next\n",
        "\n",
        "An even better way of looking at the data is through Gaussian processes. If there is periodicity, or at least some continuity in the data, this will analyse it better. For a quick intro, see https://katbailey.github.io/post/gaussian-processes-for-dummies/\n",
        "\n",
        "The TFP tutorial code also shows this in a practical example, using the VariationalGaussianProcess layer here:\n",
        "\n",
        "<a target=\"_blank\" href=\"https://colab.research.google.com/github/tensorflow/probability/blob/master/tensorflow_probability/examples/jupyter_notebooks/Probabilistic_Layers_Regression.ipynb#scrollTo=qmgmcmMKzOH7\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />Show this tutorial in Google Colab</a>"
      ]
    }
  ]
}